---
title: "<center> Tree-based Methods </center>"
output:
  html_document:
    fig_height: 4
    highlight: pygments
    theme: spacelab
  pdf_document: default
---
<center> <h3> Sunah Park </center>

This markdown file is created by Sunah Park for extended lab exercises in the book _An Introduction to Statistical Learning with Applications in R_ [(ISLR)](https://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf). 


* * *

### Setup for code chunks
```{r mychunksetup, echo=TRUE, include=TRUE} 
rm(list=ls())
# default r markdown global options in document
knitr::opts_chunk$set(
   ########## Text results ##########
    echo=TRUE, 
    warning=FALSE, # to preserve warnings in the output 
    error=FALSE, # to preserve errors in the output
    message=FALSE, # to preserve messages
    strip.white=TRUE, # to remove the white lines in the beginning or end of a source chunk in the output 

    ########## Cache ##########
    cache=TRUE,
   
    ########## Plots ##########
    fig.path="", # prefix to be used for figure filenames
    fig.width=8,
    fig.height=6,
    dpi=200
)
```



* * * 
```{r lib, echo=TRUE, include=TRUE} 
library(ISLR)
library(tree) # Tree library to construct classification and regression trees
library(ggplot2)
library(MASS)
library(randomForest)
library(gbm) # Gradient Boosted Machine
library(caret) # confusionmatrix
```


* * *
## __1. Classification Tree__
```{r df, echo=TRUE, include=TRUE} 
head(Carseats,3); dim(Carseats)
High<-ifelse(Carseats$Sales<=8, "No","Yes")
Carseats<-data.frame(Carseats,High)
```


```{r class-tree, echo=TRUE, include=TRUE} 
tree.carseats<-tree(High~.-Sales, data=Carseats) # Fit a classification tree in order to predict High using all variables but Sales
summary(tree.carseats)

plot(tree.carseats)
text(tree.carseats, pretty=0, cex=.7) # pretty=0 to include the category names for any qualitative predictors

tree.carseats
```

We see that the training error rate is 9%. 

The most important indicator of Sales appears to be shelving location(ShelveLoc), since the first branch differentiates Good locations from Bad and Medium locations.
By typing tree.carseats R displays the split criterion, the number of observations in that branch, the deviance, the overall prediction for the branch, and the fraction of observations in that branch that take on values of Yes or No. Branches that lead to terminal nodes are indicated using asterisks.

```{r class-tree2, echo=TRUE, include=TRUE} 
set.seed(2)
train<-sample(1:nrow(Carseats),200) #splitting
High.test<-High[-train]

tree.carseats<-tree(High~.-Sales, data=Carseats[train,])
tree.pred<-predict(tree.carseats, Carseats[-train,], type="class") # The argument type="class" instructs R to return the actual class prediction.
table(tree.pred, High.test)
accuracy<-(86+57)/(86+27+30+57); accuracy

confusionMatrix(tree.pred, as.factor(High.test)) # from the caret library
```
This approach leads to correct predictions for around 71.5% of the locations in the test data set.

We now consider whether pruning the tree might lead to improved results. The function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration. We use the argument __FUN=prune.misclass__ in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance. 

The cv.tree() function reports the number of terminal nodes of each tree considered(size) as well as the corresponding error rate and the value of the cost-complexity parameter used (k).
```{r class-tree-pruning, echo=TRUE, include=TRUE} 
set.seed(3)
cv.carseats<-cv.tree(tree.carseats, FUN=prune.misclass, K=10) # FUN: The function to do the pruning(prune.missclass: classification error rate), K: The number of folds of the CV.
names(cv.carseats)
cv.carseats

par(mfrow=c(1,2))
plot(cv.carseats$size, cv.carseats$dev, type="b")
plot(cv.carseats$k, cv.carseats$dev, type="b")
```

dev corresponds to the cross-validation error rate. The tree with 9 terminal nodes results in the lowest cross-validation error rate with 50 cross-validation errors. 

```{r class-tree-pruning2, echo=TRUE, include=TRUE} 
prune.carseats<-prune.misclass(tree.carseats, best=cv.carseats$size[which.min(cv.carseats$dev)]) # prune.misclass() function in order to prune the tree to obtain the nine-node tree, prune.misclass=prune.tree(method="misclass")
plot(prune.carseats)
text(prune.carseats, pretty=0)

tree.pred<-predict(prune.carseats, Carseats[-train,], type="class")
table(tree.pred, High.test)
accuracy<-(94+60)/(94+24+22+60); accuracy

confusionMatrix(tree.pred, as.factor(High.test))
```
Now 77% of the test observations are correctly classified, so not only has the pruning process produced a more interpretable tree, but it has also improved the classification accuracy. 



```{r class-tree-pruning3, echo=TRUE, include=TRUE} 
prune.carseats<-prune.misclass(tree.carseats, best=15) # increased value of best 
plot(prune.carseats)
text(prune.carseats, pretty=0)

tree.pred<-predict(prune.carseats, Carseats[-train,],type="class")
table(tree.pred, High.test)
accuracy<-(86+62)/(86+22+30+62); accuracy

confusionMatrix(tree.pred, as.factor(High.test))
```

We obtain a larger pruned tree with lower classification accuracy(74%). 

* * *
## __2. Regression Tree__
```{r reg-tree, echo=TRUE, include=TRUE} 
set.seed(1)
train<-sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston<-tree(medv~., Boston[train,])
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty=0, cex=.7)
```

We now use the cv.tree() function to see whether pruning the tree will improve performance.
```{r reg-tree-pruning, echo=TRUE, include=TRUE} 
cv.boston<-cv.tree(tree.boston, FUN=prune.tree, K=10)
par(mfrow=c(1,2))
plot(cv.boston$size, cv.boston$dev, type="b")
# In this case, the more complex tree is selected by cross-validation. 
plot(cv.boston$k, cv.boston$dev, type="b")


par(mfrow=c(1,1))
prune.boston<-prune.tree(tree.boston, best=5) # We still want to prune the tree
plot(prune.boston)
text(prune.boston, pretty=0, cex=.7)

## MSE of Pruned tree
tree.pred<-predict(prune.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]

plot(tree.pred, boston.test)
abline(0,1)
mean((tree.pred-boston.test)^2)


## MSE of Unpruned tree
tree.pred<-predict(tree.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]

plot(tree.pred, boston.test)
abline(0,1)
mean((tree.pred-boston.test)^2)
```

The test set MSE associated with the regression tree is 25.05. The square root of the MSE is therefore around 5.005, indicating that this model leads to test predictions that are within around $5,005 of the true median home value for the suburb.
Note that the MSE of the unpruned tree is smaller than that of pruned tree, which can be explained from the cross-validation error plot. 


* * *
## __3. Bagging and Random Forests__
Bagging is simply a special case of a random forest with m=p. Therefore, the randomForest() function can be used to perform both random forests and bagging. 

```{r dfsplit, echo=TRUE, include=TRUE} 
set.seed(1)
train<-sample(1:nrow(Boston), nrow(Boston)/2)
```

### Bagging
```{r bagging, echo=TRUE, include=TRUE} 
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,], 
                         mtry=13, # Number of variables randomly sampled as candidates at each split
                         ntree=25) # Number of trees to grow
bag.boston

bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
```

The argument mtry=13 indicates that all 13 predictors should be considered for each split of the tree - in other words, that bagging should be done. 

The test set MSE associated with the bagged regression tree is almost half that obtained using an pruned single tree.

### Random Forest
Growing a random forest proceeds in exactly the same way, except that we use a smaller value of the mtry argument. By default, randomForest() uses __p/3 variables__ when building a random forest of __regression trees__, and __sqrt(p) variables__ when building a random forest of __classification trees__. Here we use mtry=6. 
```{r rf, echo=TRUE, include=TRUE} 
set.seed(1)
rf.boston<-randomForest(medv~., data=Boston[train,], 
                         mtry=6, # Number of variables randomly sampled as candidates at each split
                         ntree=25,
                         importance=TRUE) # Number of trees to grow
rf.boston

rf.pred<-predict(rf.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(rf.pred, boston.test)
mean((rf.pred-boston.test)^2)
```

The random forests yield an improvement over bagging in terms of MSE. 

Using the importance function, we can view the importance of each variable. Two measures of variable importance are reported. The former is based upon the mean decrease of accuracy in predictions on the out of bag samples when a given variable is excluded from the model. The latter is a measure of the total increase in node impurity that results from splits over that variable, averaged over all trees. In the case of regression trees, the node impurity is measured by the training RSS, and for classification trees by the deviance. Plots of these importance measures can be produced using the varImpPlot() function.
```{r importance, echo=TRUE, include=TRUE} 
importance(rf.boston)
varImpPlot(rf.boston)
```


The results indicate that across all of the trees considered in the random forest, the lstat (the wealth level of the community) and the house size(rm) are by far the two most important variables.

* * *
## __4. Boosting__
We run gbm() with the option __distribution="gaussian"__ since this is a __regression__ problem; if it were a __binary classification__ problem, we would use __distribution="bernoulli"__.
```{r boosting, echo=TRUE, include=TRUE} 
set.seed(1)
boost.boston<-gbm(medv~., data=Boston[train,], 
                  distribution="gaussian",
                  n.trees=5000, # number of trees (default:100)
                  interaction.depth=4) # the depth of each tree

summary(boost.boston)

boost.pred<-predict(boost.boston, newdata=Boston[-train,], 
                    n.trees=5000)
mean((boost.pred-boston.test)^2)


## shrinkage parameter (learning rate)
boost.boston<-gbm(medv~., data=Boston[train,], 
                  distribution="gaussian",
                  n.trees=5000, # number of trees (default:100)
                  interaction.depth=4, # the maximum depth of each tree (the highest level of variable interactions allowed). A value of 1 implies an additive model, a value of 2 implies a model with up to 2-way interactions. Default is 1.
                  shrinkage=0.2, # shrinkage parameter applied to each tree in the expansion. Also know as the learning rate or step-size reduction. A smaller learning rate typically requires more trees. Default is 0.1
                  verbose=F)

boost.pred<-predict(boost.boston, newdata=Boston[-train,], 
                    n.trees=5000)
mean((boost.pred-boston.test)^2)

```
We see that lstat and rm are by far the most important variables. We can also produce partial dependence plots for these two variables. These plots illustrate the marginal effect of the selected variables on the response after integrating out the other variables. The median house prices are increasing with rm and decreasing with lstat.
```{r boosting-plot, echo=TRUE, include=TRUE} 
par(mfrow=c(1,2))
plot(boost.boston, i="rm")
plot(boost.boston, i="lstat")
```
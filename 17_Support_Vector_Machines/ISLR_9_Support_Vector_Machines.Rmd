---
title: "<center> Support Vector Machines </center>"
output:
  html_document:
    fig_height: 4
    highlight: pygments
    theme: spacelab
  pdf_document: default
---
<center> <h3> Sunah Park </center>

This markdown file is created by Sunah Park for extended lab exercises in the book _An Introduction to Statistical Learning with Applications in R_ [(ISLR)](https://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf). 


* * *

### Setup for code chunks
```{r mychunksetup, echo=TRUE, include=TRUE} 
rm(list=ls())
# default r markdown global options in document
knitr::opts_chunk$set(
   ########## Text results ##########
    echo=TRUE, 
    warning=FALSE, # to preserve warnings in the output 
    error=FALSE, # to preserve errors in the output
    message=FALSE, # to preserve messages
    strip.white=TRUE, # to remove the white lines in the beginning or end of a source chunk in the output 

    ########## Cache ##########
    cache=TRUE,
   
    ########## Plots ##########
    fig.path="", # prefix to be used for figure filenames
    fig.width=8,
    fig.height=6,
    dpi=200
)
```



* * * 
```{r lib, echo=TRUE, include=TRUE} 
library(ISLR)
library(e1071) # svm() function
library(ggplot2)
library(caret)
library(ROCR) # rocplot() function
```


* * *
## __1. Suppert Vector Classifier__
```{r df, echo=TRUE, include=TRUE} 
set.seed(1)
x<-matrix(rnorm(20*2), ncol=2)
y<-c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,]+1

plot(x, col=(3-y)) # check whether the classes are linearly separable -> They are not separable.
dat<-data.frame(x=x, y=as.factor(y))

```

```{r svm, echo=TRUE, include=TRUE} 
svmfit<-svm(y~., data=dat, kernel="linear", cost=10, scale=FALSE) # kernel: linear/polynomial/radial basis/sigmoid, cost: cost of constraints violation (default:1) - "C" constant of the regularization term in the Lagrange formulation
plot(svmfit,dat) # -1: blue, 1: purple

svmfit$index
summary(svmfit)
```

The decision boundary between the two classes is linear (Because we used the argument kernel="linear"), though due to the way in which the plotting function is implemented in this library the decision boundary looks somewhat jagged in the plot. Only one observation is misclassified. The support vectors are plotted as crosses and the remaining observations are plotted as circles. There are seven support vectors (svmfit$index). A linear kernel was used with cost=10 and there were seven support vectors, four in one class and three in the other. 

What if we instead used a smaller value of the cost parameter?
```{r svm2, echo=TRUE, include=TRUE} 
svmfit<-svm(y~., data=dat, kernel="linear", cost=0.1, scale=FALSE)
plot(svmfit,dat) # -1: blue, 1: purple

svmfit$index
summary(svmfit)
```

A smaller value of the cost parameter is being used, we obtain a larger number of support vectors, because the margin is now wider. The svm() function does not explicitly output the coefficients of the linear decision boundary obtained when the support vector classifier is fit, nor does it output the width of the margin. 

The e1071 library includes a built-in function, tune(), to perform cross-validation (by default 10-folds CV).

```{r svm-cv, echo=TRUE, include=TRUE} 
set.seed(1)
tune.out<-tune(svm, y~., data=dat, kernel="linear",
               ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100))) # ranges: list of parameter vectors spanning the sampling space
summary(tune.out)

plot(tune.out)
tune.out$best.parameters

bestmod<-tune.out$best.model
summary(bestmod)
```

We see that cost=0.1 results in the lowest cross-validation error rate. The tune() function stores the best model obtained by best.model. 

```{r svm-pred, echo=TRUE, include=TRUE} 
set.seed(1)
xtest<-matrix(rnorm(20*2), ncol=2)
ytest<-sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1,]+1

testdat<-data.frame(x=xtest, y=as.factor(ytest))
ypred<-predict(bestmod, testdat)
table(ypred, testdat$y)
confusionMatrix(ypred, testdat$y)
```

With the value of cost=0.1, 18 of the test observations are correctly classified. What if we had instead used cost=0.01?

```{r svm-pred2, echo=TRUE, include=TRUE} 
svmfit<-svm(y~., data=dat, kernel="linear", cost=0.01, scale=FALSE)
ypred<-predict(svmfit, testdat)
confusionMatrix(ypred, testdat$y)
```
In this case three additional observation is misclassified.

Now we consider a situation in which two classes are linearly separable. We can find a separating hyperplane using the svm() function. We fit the support vector classifier and plot the resulting hyperplane, using a very large value of cost so that no observations are misclassified (cost=1e5)

```{r svm3, echo=TRUE, include=TRUE} 
x[y==1,]=x[y==1,]+0.5
plot(x, col=(y+5)/2, pch=19)

dat<-data.frame(x=x, y=as.factor(y))
svmfit<-svm(y~., data=dat, kernel="linear", cost=1e5)
summary(svmfit)
plot(svmfit, dat)
```

No training errors were made and only three support vectors were used. However, we can see from the figure that the margin is very narrow (because the observations that are not support vectors, indicated as circles, are very close to the decision boundary). It seems likely that this model will perform poorly on test data. 
We now try a smaller value of cost(cost=1).
```{r svm4, echo=TRUE, include=TRUE} 
svmfit<-svm(y~., data=dat, kernel="linear", cost=1)
summary(svmfit)
plot(svmfit, dat)
```

Using cost=1, we misclassify a training observation, but we also obtain a much wider margin and make use of seven support vectors. It seems likely that this model will perform better on test data than the model with cost=1e5.

* * *
## __2. Support Vector Machine__
In order to fit an SVM using a nonlinear kernel, we use a different value of the parameter kernel. To fit an SVM with a polynomial kernel we use kernel="polynomial" with degree argument. To fit an SVM with a radial kernel we use kernel="radial" with gamma argument.
```{r df2, echo=TRUE, include=TRUE} 
set.seed(1)
x<-matrix(rnorm(200*2), ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y<-c(rep(1,150), rep(2,50))
dat<-data.frame(x=x, y=as.factor(y))
plot(x,col=(y)) # The class boundary is non-linear
```

```{r svm6, echo=TRUE, include=TRUE} 
train<-sample(200,100)
svmfit<-svm(y~., data=dat[train,], kernel="radial", gamma=1, cost=1)
plot(svmfit, dat[train,])
summary(svmfit)
```
The plot shows that the resulting SVM has a decidely non-linear boundary. 

We can see from the figure that there are a fair number of training errors in this SVM fit. If we increase the value of cost, we can reduce the number of training errors. However, this comes at the price of a more irregular decision boundary that seems to be at risk of overfitting the data. 
```{r svm5, echo=TRUE, include=TRUE} 
svmfit<-svm(y~., data=dat[train,], kernel="radial", gamma=1, cost=1e5)
plot(svmfit, dat[train,])
summary(svmfit)
```

We can perform cross-validation using tune() to select the best choice of gamma and cost for an SVM with a radial kernel.
```{r svm-cv2, echo=TRUE, include=TRUE} 
set.seed(1)
tune.out<-tune(svm, y~., data=dat[train,], kernel="radial",
               ranges=list(
                   cost=c(0.1, 1, 10, 100, 1000),
                   gamma=c(0.5, 1,2,3,4)))
summary(tune.out)
tune.out$best.parameters

svm.pred<-predict(tune.out$best.model, newx=dat[-train,])
table(dat[-train,"y"], svm.pred)
confusionMatrix(dat[-train,"y"], svm.pred)
```

The best choice of parameters involves cost=1 and gamma=2. By this SVM 39% of test observations are misclassified. 

* * *
## __3. ROC Curves__
```{r ROC, echo=TRUE, include=TRUE} 
rocplot<-function(pred, truth, ...) {
    predob<-prediction(pred, truth)
    perf<-performance(predob, "tpr", "fpr")
    plot(perf, ...)}
```

SVMs and support vector classifiers output class labels for each observation. However, it is also possible to obtain fitted values for each observation, which are the numerical scores used to obtain the class labels. In essence, the sign of the fitted value determines on which side of the decision boundary the observation lies. Therefore, the relationship between the fitted value and the class prediction for a given observation is simple: if the fitted value exceeds zero then the observation is assigned to one class, and if it is less than zero then it is assigned to the other. In order to obtain the fitted values for a given SVM model fit, we use decision.values=TRUE when fitting svm(). Then the predict() function will output the fitted values. 
```{r ROC2, echo=TRUE, include=TRUE} 
svmfit.opt<-svm(y~., data=dat[train,], kernel="radial", gamma=2, cost=1, decision.values=TRUE) 
svm.pred1<-predict(svmfit.opt, dat[train,], decision.values=TRUE)
svm.pred2<-predict(svmfit.opt, dat[-train,], decision.values=TRUE)

fitted1<-attributes(svm.pred1)$decision.values
fitted2<-attributes(svm.pred2)$decision.values

par(mfrow=c(1,2))
rocplot(fitted1, dat[train,"y"], main="Training Data") # ROC plot
rocplot(fitted2, dat[train,"y"], main="Test Data", col="red") # ROC plot

```


* * *
## __4. SVM with multiple classes__
If the response is a factor containing more than two levels, then the svm() function will perform multi-class classification using the one-versus-one approach. 
```{r svm-m, echo=TRUE, include=TRUE} 
set.seed(1)
x<-rbind(x, matrix(rnorm(50*2), ncol=2))
y<-c(y, rep(0,50))
x[y==0,2]=x[y==0,2]+2
dat<-data.frame(x=x, y=as.factor(y))
plot(x, col=y+1, pch=19)

svmfit<-svm(y~., data=dat, kernel="radial", cost=10, gamma=1)
plot(svmfit, dat)
```

We now examine the Khan data set, which consists of a number of tissue samples corresponding to four distinct types of small round blue cell tumors. The data set consists of training data, xtrain and ytrain and testing data, xtest and ytest.
```{r svm-m-df, echo=TRUE, include=TRUE} 
names(Khan)
table(Khan$ytrain)
table(Khan$ytest)
```

```{r svm-m2, echo=TRUE, include=TRUE} 
dat<-data.frame(x=Khan$xtrain, y=as.factor(Khan$ytrain))
out<-svm(y~., data=dat, kernel="linear", cost=10)
summary(out)

table(out$fitted, dat$y)
confusionMatrix(out$fitted, dat$y)
```

We see that there are no training erros. This is not surprising, because the large number of variables relative to the number of observations implies that it is easy to find hyperplanes that fully separate the classes. 

We are most interested not in the support vector classifier's performance on the training observations, but rather its performance on the test observations.
```{r svm-m3, echo=TRUE, include=TRUE} 
dat<-data.frame(x=Khan$xtrain, y=as.factor(Khan$ytrain))
out<-svm(y~., data=dat, kernel="linear", cost=10)
dat.test<-data.frame(x=Khan$xtest, y=as.factor(Khan$ytest))

pred<-predict(out, newdata=dat.test)
table(pred, dat.test$y)
confusionMatrix(out$fitted, dat$y)
```

We see that using cost=10 yields two test set errors on this data.
fig.path="", # prefix to be used for figure filenames
fig.width=8,
fig.height=6,
dpi=200
)
library(ISLR)
library(glmnet) # glmnet() function
library(ggplot2)
head(Hitters,3)
Hitters<-na.omit(Hitters)
x<-model.matrix(Salary~., data=Hitters)[,-1]
y<-Hitters$Salary
grid<-10^seq(10,-2, length=100)
ridge.mod<-glmnet(x,y,alpha=0, lambda=grid, standardize=TRUE)
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
predict(ridge.mod, s=50, type="coefficients")[1:20, ] # Ridge regression coefficient for lambda=50
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
predict(ridge.mod, s=60, type="coefficients")[1:20, ] # Ridge regression coefficient for lambda=60
set.seed(1)
train<-sample(1:nrow(x),size=nrow(x)/2)
test<--train
y.test<-y[test]
ridge.mod<-glmnet(x=x[train,], y=y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred<-predict(ridge.mod, s=4, newx=x[test,])
mean((ridge.pred-y.test)^2) # MSE
ridge.mod<-glmnet(x=x[train,], y=y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred<-predict(ridge.mod, s=10e10, newx=x[test,])
mean((ridge.pred-y.test)^2) # MSE
ridge.mod<-glmnet(x=x[train,], y=y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred<-predict(ridge.mod, s=0, newx=x[test,], exact=TRUE)
ridge.mod<-glmnet(x=x[train,], y=y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred<-predict(ridge.mod, s=0, newx=x[test,])
mean((ridge.pred-y.test)^2) # MSE
ridge.mod<-glmnet(x=x[train,], y=y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred<-predict(ridge.mod, s=0, newx=x[test,])
mean((ridge.pred-y.test)^2) # MSE
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10, type.measure="mse") # 10-folds CV
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlamda)
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10, type.measure="mse") # 10-folds CV
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlamda)
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10, type.measure="mse") # 10-folds CV
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlambda)
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10, type.measure="mse") # 10-folds CV
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlambda)
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10, type.measure="mse") # 10-folds CV
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlambda)
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10, type.measure="mse") # 10-folds CV
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlambda)
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10, type.measure="mse") # 10-folds CV
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlambda)
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10, type.measure="mse") # 10-folds CV
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlambda)
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10, type.measure="mse") # 10-folds CV
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlambda) [1:20,]
mean((ridge.pred-y.test)^2)
head(Hitters,3)
Hitters<-na.omit(Hitters)
x<-model.matrix(Salary~., data=Hitters)[,-1]
y<-Hitters$Salary
View(Hitters)
View(x)
View(x)
grid<-10^seq(10,-2, length=100)
ridge.mod<-glmnet(x,y,alpha=0, lambda=grid, standardize=TRUE)
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
predict(ridge.mod, s=50, type="coefficients")[1:20, ] # Ridge regression coefficient for lambda=50
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
predict(ridge.mod, s=60, type="coefficients")[1:20, ] # Ridge regression coefficient for lambda=60
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
predict(ridge.mod, s=50, type="coefficients")[1:20, ] # Ridge regression coefficient for lambda=50
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
predict(ridge.mod, s=60, type="coefficients")[1:20, ] # Ridge regression coefficient for lambda=60
set.seed(1)
train<-sample(1:nrow(x),size=nrow(x)/2)
test<--train
y.test<-y[test]
ridge.mod<-glmnet(x=x[train,], y=y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred<-predict(ridge.mod, s=0, newx=x[test,])
mean((ridge.pred-y.test)^2) # MSE
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10, type.measure="mse") # 10-folds CV
plot(cv.out)
summary(cv.out)
?cv.glmnet
summary(cv.out)$cvm
summary(cv.out)
cv.out
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10, type.measure="mse") # 10-folds CV
cv.out
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10, type.measure="mse") # 10-folds CV
cv.out
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10,lambda=grid, type.measure="mse") # 10-folds CV
cv.out
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlambda) [1:20,]
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10,lambda=10, type.measure="mse") # 10-folds CV
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10,lambda=100, type.measure="mse") # 10-folds CV
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10,lambda=1:100, type.measure="mse") # 10-folds CV
cv.out
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlambda) [1:20,]
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10,lambda=grid, type.measure="mse") # 10-folds CV
cv.out
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlambda) [1:20,]
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10,lambda=NULL, type.measure="mse") # 10-folds CV
cv.out
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlambda) [1:20,]
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10,lambda=seq(0.01,1000, by=0.2), type.measure="mse") # 10-folds CV - lambda default is null and glmnet chooses its own sequence
cv.out
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlambda) [1:20,]
cv.out
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10,lambda=NULL, type.measure="mse") # 10-folds CV - lambda default is null and glmnet chooses its own sequence
cv.out
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlambda) [1:20,]
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
ridge.pred<-predict(ridge.mod, s=4, newx=x[test,])
mean((ridge.pred-y.test)^2)
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2)
?glmnet
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error)
predict(out, type="coefficients", s=bestlambda)
predict(out, type="coefficients", s=bestlambda) [1:20,]
?cv.glmnet
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error: MSE in this case), The value of lambda that results in the smallest cv error is 212.
1selambda<-cv.out$lambda.1se; 1selambda # lambda that gives minimum cvm (mean cross-validated error: MSE in this case), The value of lambda that results in the smallest cv error is 212.
1selambda<-cv.out$lambda.1se;
selambda<-cv.out$lambda.1se;
selambda # lambda that gives minimum cvm (mean cross-validated error: MSE in this case), The value of lambda that results in the smallest cv error is 212.
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error: MSE in this case), The value of lambda that results in the smallest cv error is 212.
selambda<-cv.out$lambda.1se; selambda # largest value of lambda such that error is within 1 standard error of the minimum
?predict
?predict
out<-glmnet(x,y, alpha=0) # fitted "glmnet" model object
predict(object=out, type="coefficients", s=bestlambda) [1:20,]  # Type "coefficients" computes the coefficients at the requested values for s. newx(Matrix of new values for x at which predictions are to be made) is not used for "coefficients".
lasso.mod<-glmnet(x=x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
ridge.mod<-glmnet(x=x[train,], y=y[train], alpha=0, lambda=grid, thresh=1e-12)
plot(ridge.mod)
ridge.mod<-glmnet(x=x[train,], y=y[train], alpha=0, lambda=grid, thresh=1e-12)
plot(ridge.mod)
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=1, nfolds=10, lambda=NULL, type.measure="mse")
plot(cv.out)
bestlambda<-cv.out$lambda.min
lasso.pred<-predict(lasso.mod, s=bestlambda, newx=x[test,])
mean((lasso.pred-y.test)^2)
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=1, nfolds=10, lambda=NULL, type.measure="mse")
plot(cv.out)
bestlambda<-cv.out$lambda.min
lasso.pred<-predict(lasso.mod, s=bestlambda, newx=x[test,])
mean((lasso.pred-y.test)^2)
mean((lasso.pred-y.test)^2)
mean((ridge.pred-y.test)^2)
ridge.mod<-glmnet(x=x[train,], y=y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred<-predict(ridge.mod, s=0, newx=x[test,])
mean((ridge.pred-y.test)^2) # MSE
ridge.pred<-predict(ridge.mod, s=0, newx=x[test,])
mean((ridge.pred-y.test)^2) # MSE
bestlambda<-cv.out$lambda.min
bestlambda<-cv.out$lambda.min; bestlambda
mean((lasso.pred-y.test)^2)
mean((ridge.pred-y.test)^2)
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10,lambda=NULL, type.measure="mse") # 10-folds CV - lambda default is null and glmnet chooses its own sequence
cv.out
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error: MSE in this case), The value of lambda that results in the smallest cv error is 212.
selambda<-cv.out$lambda.1se; selambda # largest value of lambda such that error is within 1 standard error of the minimum. In this case 7972.
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2) # MSE associated with the best lambda value
out<-glmnet(x,y, alpha=0) # fitted "glmnet" model object
predict(object=out, type="coefficients", s=bestlambda) [1:20,]  # Type "coefficients" computes the coefficients at the requested values for s. newx(Matrix of new values for x at which predictions are to be made) is not used for "coefficients".
mean((ridge.pred-y.test)^2) # MSE associated with the best lambda value
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=1, nfolds=10, lambda=NULL, type.measure="mse")
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda
lasso.pred<-predict(lasso.mod, s=0, newx=x[test,])
mean((lasso.pred-y.test)^2)
mean((ridge.pred-y.test)^2)
mean((lasso.pred-y.test)^2)
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=1, nfolds=10, lambda=NULL, type.measure="mse")
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda
lasso.pred<-predict(lasso.mod, s=bestlambda, newx=x[test,])
mean((lasso.pred-y.test)^2)
mean((ridge.pred-y.test)^2)
mean((lasso.pred-y.test)^2)
mean((ridge.pred-y.test)^2)
out<-glmnet(x,y,alpha=1, lambda=grid)
lasso.coef<-predict(out, type="coefficients", s=bestlambda)[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]
out<-glmnet(x,y, alpha=0) # fitted "glmnet" model object
ridge.coef<-predict(object=out, type="coefficients", s=bestlambda) [1:20,]  # Type "coefficients" computes the coefficients at the requested values for s. newx(Matrix of new values for x at which predictions are to be made) is not used for "coefficients".
ridge.coef
ridge.coef[ridge.coef!=0]
lasso.coef[lasso.coef!=0]
ridge.coef[ridge.coef!=0]
length(lasso.coef[lasso.coef!=0])
length(ridge.coef[ridge.coef!=0])
length(lasso.coef[lasso.coef!=0])-1
length(ridge.coef[ridge.coef!=0])-1
out<-glmnet(x,y,alpha=1, lambda=grid)
lasso.coef<-predict(out, type="coefficients", s=bestlambda)[1:20,]
lasso.coef
length(lasso.coef[lasso.coef!=0])-1
View(x)
length(cv.out)
dim(cv.out)
length(cv.out$lambda)
range(cv.out$lambda)
range(log(cv.out$lambda))
range(log(cv.out$cvm))
cv.out$cvm
which.min(cv.out$cvm)
plot(cv.out$cvm)
plot(cv.out$cvm~cv.out$lambda)
plot(cv.out$cvm)
plot(y=cv.out$cvm)
plot(y=cv.out$cvm, x=cv.out$cvm)
plot(y=cv.out$cvm, x=cv.out$lambda)
cv.out$lambda
plot(y=cv.out$cvm, x=cv.out$lambda)
plot(cv.out$cvm)
plot(cv.out$cvm~cv.out$lambda)
plot(cv.out$cvm~log(cv.out$lambda))
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10,lambda=NULL, type.measure="mse") # 10-folds CV - lambda default is null and glmnet chooses its own sequence
plot(cv.out$cvm~log(cv.out$lambda))
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error: MSE in this case), The value of lambda that results in the smallest cv error is 212.
selambda<-cv.out$lambda.1se; selambda # largest value of lambda such that error is within 1 standard error of the minimum. In this case 7972.
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2) # MSE associated with the best lambda value
out<-glmnet(x,y, alpha=0) # fitted "glmnet" model object
ridge.coef<-predict(object=out, type="coefficients", s=bestlambda) [1:20,]  # Type "coefficients" computes the coefficients at the requested values for s. newx(Matrix of new values for x at which predictions are to be made) is not used for "coefficients".
ridge.coef
ridge.coef[ridge.coef!=0]
plot(cv.out)
plot(cv.out$cvm~log(cv.out$lambda))
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10,lambda=NULL, type.measure="mse") # 10-folds CV - lambda default is null and glmnet chooses its own sequence
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error: MSE in this case), The value of lambda that results in the smallest cv error is 212.
selambda<-cv.out$lambda.1se; selambda # largest value of lambda such that error is within 1 standard error of the minimum. In this case 7972.
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2) # MSE associated with the best lambda value
out<-glmnet(x,y, alpha=0) # fitted "glmnet" model object
ridge.coef<-predict(object=out, type="coefficients", s=bestlambda) [1:20,]  # Type "coefficients" computes the coefficients at the requested values for s. newx(Matrix of new values for x at which predictions are to be made) is not used for "coefficients".
ridge.coef
ridge.coef[ridge.coef!=0]
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error: MSE in this case), The value of lambda that results in the smallest cv error is 212.
plot(cv.out$cvm~cv.out$lambda)
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10,lambda=NULL, type.measure="mse") # 10-folds CV - lambda default is null and glmnet chooses its own sequence
plot(cv.out)
plot(cv.out$cvm~cv.out$lambda)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error: MSE in this case), The value of lambda that results in the smallest cv error is 212.
selambda<-cv.out$lambda.1se; selambda # largest value of lambda such that error is within 1 standard error of the minimum. In this case 7972.
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2) # MSE associated with the best lambda value
out<-glmnet(x,y, alpha=0) # fitted "glmnet" model object
ridge.coef<-predict(object=out, type="coefficients", s=bestlambda) [1:20,]  # Type "coefficients" computes the coefficients at the requested values for s. newx(Matrix of new values for x at which predictions are to be made) is not used for "coefficients".
ridge.coef
ridge.coef[ridge.coef!=0]
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10,lambda=NULL, type.measure="mse") # 10-folds CV - lambda default is null and glmnet chooses its own sequence
plot(cv.out)
plot(y=cv.out$cvm,x=cv.out$lambda)
plot(y=cv.out$cvm,x=log(cv.out$lambda))
log221
log(221)
selambda<-cv.out$lambda.1se; selambda # largest value of lambda such that error is within 1 standard error of the minimum. In this case 7972.
set.seed(1)
cv.out<-cv.glmnet(x[train,], y[train], alpha=0, nfolds=10,lambda=NULL, type.measure="mse") # 10-folds CV - lambda default is null and glmnet chooses its own sequence
plot(cv.out)
bestlambda<-cv.out$lambda.min; bestlambda # lambda that gives minimum cvm (mean cross-validated error: MSE in this case), The value of lambda that results in the smallest cv error is 212.
selambda<-cv.out$lambda.1se; selambda # largest value of lambda such that error is within 1 standard error of the minimum. In this case 7972.
ridge.pred<-predict(ridge.mod, s=bestlambda, newx=x[test,])
mean((ridge.pred-y.test)^2) # MSE associated with the best lambda value
out<-glmnet(x,y, alpha=0) # fitted "glmnet" model object
ridge.coef<-predict(object=out, type="coefficients", s=bestlambda) [1:20,]  # Type "coefficients" computes the coefficients at the requested values for s. newx(Matrix of new values for x at which predictions are to be made) is not used for "coefficients".
ridge.coef
ridge.coef[ridge.coef!=0]
ridge.mod<-glmnet(x=x[train,], y=y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred<-predict(ridge.mod, s=0, newx=x[test,])
mean((ridge.pred-y.test)^2) # MSE
ridge.coef
ridge.coef[ridge.coef!=0]
length(ridge.coef[ridge.coef!=0])
lasso.mod<-glmnet(x=x[train,], y[train], alpha=1, lambda=grid) # For Lasso, set alpha=1
plot(lasso.mod)
bestlambda<-cv.out$lambda.min; bestlambda
mean((lasso.pred-y.test)^2)
mean((ridge.pred-y.test)^2)
mean((lasso.pred-y.test)^2)
mean((ridge.pred-y.test)^2)
length(lasso.coef[lasso.coef!=0])-1
length(ridge.coef[ridge.coef!=0])-1
library(pls)
install.packages("pls")
library(pls)
library(ggplot2)
head(Hitters,3)
Hitters<-na.omit(Hitters)
set.seed(2)
pcr.fit<-pcr(Salary~., data=Hitters, scale=TRUE, validation="CV")
summary(pcr.fit)
```{r pcr-plot, echo=TRUE, include=TRUE}
validationplot(pcr.fit, val.type="MSEP")
which.min(pcr.fit$scores)
validationplot(pcr.fit, val.type="MSEP")
which.min(pcr.fit$residuals)
validationplot(pcr.fit, val.type="MSEP")
summary(pcr.fit)
set.seed(1)
train<-sample(1:nrow(Hitters),size=nrow(Hitters)/2)
pcr.fit<-pcr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
set.seed(1)
train<-sample(1:nrow(Hitters),size=nrow(Hitters)/2)
pcr.fit<-pcr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
set.seed(1)
train<-sample(c(T,F), nrow(Hitters), rep=T)
pcr.fit<-pcr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
set.seed(1)
train<-sample(c(T,F), nrow(Hitters), rep=T)
pcr.fit<-pcr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
validationplot(pcr.fit, val.type="MSEP")
pcr.fit$ncomp
pcr.fit$ncomp
pcr.fit$Xvar
pcr.fit$projection
pcr.fit$residuals
summary(pcr.fit)
set.seed(1)
train<-sample(1:nrow(Hitters),size=nrow(Hitters)/2)
pcr.fit<-pcr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
validationplot(pcr.fit, val.type="MSEP")
summary(pcr.fit)
summary(pcr.fit)
summary(pcr.fit)$CV
pcr.fit$Xvar
pcr.fit$MSE
names(pcr.fit)
?pcr
train<-sample(1:nrow(Hitters),size=nrow(Hitters)/2)
pcr.fit<-pcr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
validationplot(pcr.fit, val.type="MSEP")
?validationplot
pcr.fit$scores
which.min(pcr.fit)
which.min(pcr.fit$residuals)
validationplot(pcr.fit, val.type="MSEP")
validationplot(pcr.fit, val.type="MSEP", lty="b")
validationplot(pcr.fit, val.type="MSEP", lty="l")
validationplot(pcr.fit, val.type="MSEP", type="l")
validationplot(pcr.fit, val.type="MSEP", type="b")
x<-model.matrix(Salary~., data=Hitters)[,-1]
y<-Hitters$Salary
set.seed(1)
train<-sample(1:nrow(x),size=nrow(x)/2)
test<--train
y.test<-y[test]
pcr.fit<-pcr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
validationplot(pcr.fit, val.type="MSEP", type="b")
summary(pcr.fit)
x<-model.matrix(Salary~., data=Hitters)[,-1]
y<-Hitters$Salary
set.seed(1)
train<-sample(1:nrow(x),size=nrow(x)/2)
test<--train
y.test<-y[test]
pcr.fit<-pcr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
validationplot(pcr.fit, val.type="MSEP", type="b")
summary(pcr.fit)
pcr.pred<-predict(pcr.fit, x[test,], ncomp=7)
mean((pcr.pred-y.test)^2)
summary(pcr.fit)
pcr.fit<-pcr(y~x, scale=TRUE, ncomp=7)
summary(pcr.fit)
set.seed(1)
pls.fit<-plsr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
summary(pls.fit)
set.seed(1)
pls.fit<-plsr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
summary(pls.fit)
validationplot(pls.fit, val.type="MSEP")
set.seed(1)
pls.fit<-plsr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
summary(pls.fit)
validationplot(pls.fit, val.type="MSEP", type="b")
pls.pred<-predict(pls.fit, x[test,], ncomp=2)
mean((pls.pred-y.test)^2)
pcr.pred<-predict(pcr.fit, x[test,], ncomp=7)
mean((pcr.pred-y.test)^2)
pls.fit<-plsr(y~x, scale=TRUE, ncomp=2)
summary(pls.fit)
pls.fit<-plsr(y~x, scale=TRUE, ncomp=2)
summary(pls.fit)

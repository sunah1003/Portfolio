train<-sample(1:nrow(Boston), nrow(Boston)/2)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,],
mtry=13, # Number of variables randomly sampled as candidates at each split
ntree=25) # Number of trees to grow
bag.boston
bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,],
mtry=13, # Number of variables randomly sampled as candidates at each split
ntree=25) # Number of trees to grow
bag.boston
bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25) # Number of trees to grow
bag.boston
bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
bag.boston
plot(bag.boston)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25) # Number of trees to grow
bag.boston
bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
summary(bag.boston)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,],
mtry=13, # Number of variables randomly sampled as candidates at each split
ntree=25) # Number of trees to grow
bag.boston
bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25) # Number of trees to grow
bag.boston
bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,],
#mtry=13, # Number of variables randomly sampled as candidates at each split
ntree=25) # Number of trees to grow
bag.boston
bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,],
mtry=13, # Number of variables randomly sampled as candidates at each split
ntree=25) # Number of trees to grow
bag.boston
bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25) # Number of trees to grow
bag.boston
bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,],
mtry=13, # Number of variables randomly sampled as candidates at each split
ntree=25) # Number of trees to grow
bag.boston
bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25) # Number of trees to grow
bag.boston
bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25,
importance=TRUE) # Number of trees to grow
bag.boston
bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
set.seed(1)
rf.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25,
importance=TRUE) # Number of trees to grow
rf.boston
rf.pred<-predict(rf.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
importance(rf.boston)
set.seed(1)
rf.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25,
importance=TRUE) # Number of trees to grow
rf.boston
rf.pred<-predict(rf.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(rf.pred, boston.test)
mean((rf.pred-boston.test)^2)
importance(rf.boston)
set.seed(1)
rf.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25,
importance=TRUE) # Number of trees to grow
rf.boston
rf.pred<-predict(rf.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(rf.pred, boston.test)
mean((rf.pred-boston.test)^2)
importance(rf.boston)
set.seed(1)
rf.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25,
importance=TRUE) # Number of trees to grow
rf.boston
rf.pred<-predict(rf.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(rf.pred, boston.test)
mean((rf.pred-boston.test)^2)
importance(rf.boston)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,],
mtry=13, # Number of variables randomly sampled as candidates at each split
ntree=25) # Number of trees to grow
bag.boston
bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
cv.boston<-cv.tree(tree.boston, FUN=prune.tree, K=10)
---
title: "<center> Tree-based Methods </center>"
output:
html_document:
fig_height: 4
highlight: pygments
theme: spacelab
pdf_document: default
---
<center> <h3> Sunah Park </center>
This markdown file is created by Sunah Park for extended lab exercises in the book _An Introduction to Statistical Learning with Applications in R_ [(ISLR)](https://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf).
* * *
### Setup for code chunks
```{r mychunksetup, echo=TRUE, include=TRUE}
rm(list=ls())
# default r markdown global options in document
knitr::opts_chunk$set(
########## Text results ##########
echo=TRUE,
warning=FALSE, # to preserve warnings in the output
error=FALSE, # to preserve errors in the output
message=FALSE, # to preserve messages
strip.white=TRUE, # to remove the white lines in the beginning or end of a source chunk in the output
########## Cache ##########
cache=TRUE,
########## Plots ##########
fig.path="", # prefix to be used for figure filenames
fig.width=8,
fig.height=6,
dpi=200
)
```
* * *
```{r lib, echo=TRUE, include=TRUE}
library(ISLR)
library(tree) # Tree library to construct classification and regression trees
library(ggplot2)
library(MASS)
library(randomForest)
library(caret) # confusionmatrix
```
* * *
## __1. Classification Tree__
```{r df, echo=TRUE, include=TRUE}
head(Carseats,3); dim(Carseats)
High<-ifelse(Carseats$Sales<=8, "No","Yes")
Carseats<-data.frame(Carseats,High)
```
```{r class-tree, echo=TRUE, include=TRUE}
tree.carseats<-tree(High~.-Sales, data=Carseats) # Fit a classification tree in order to predict High using all variables but Sales
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty=0, cex=.7) # pretty=0 to include the category names for any qualitative predictors
tree.carseats
```
We see that the training error rate is 9%.
The most important indicator of Sales appears to be shelving location(ShelveLoc), since the first branch differentiates Good locations from Bad and Medium locations.
By typing tree.carseats R displays the split criterion, the number of observations in that branch, the deviance, the overall prediction for the branch, and the fraction of observations in that branch that take on values of Yes or No. Branches that lead to terminal nodes are indicated using asterisks.
```{r class-tree2, echo=TRUE, include=TRUE}
set.seed(2)
train<-sample(1:nrow(Carseats),200) #splitting
High.test<-High[-train]
tree.carseats<-tree(High~.-Sales, data=Carseats[train,])
tree.pred<-predict(tree.carseats, Carseats[-train,], type="class") # The argument type="class" instructs R to return the actual class prediction.
table(tree.pred, High.test)
accuracy<-(86+57)/(86+27+30+57); accuracy
confusionMatrix(tree.pred, as.factor(High.test)) # from the caret library
```
This approach leads to correct predictions for around 71.5% of the locations in the test data set.
We now consider whether pruning the tree might lead to improved results. The function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration. We use the argument __FUN=prune.misclass__ in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance.
The cv.tree() function reports the number of terminal nodes of each tree considered(size) as well as the corresponding error rate and the value of the cost-complexity parameter used (k).
```{r class-tree-pruning, echo=TRUE, include=TRUE}
set.seed(3)
cv.carseats<-cv.tree(tree.carseats, FUN=prune.misclass, K=10) # FUN: The function to do the pruning(prune.missclass: classification error rate), K: The number of folds of the CV.
names(cv.carseats)
cv.carseats
par(mfrow=c(1,2))
plot(cv.carseats$size, cv.carseats$dev, type="b")
plot(cv.carseats$k, cv.carseats$dev, type="b")
```
dev corresponds to the cross-validation error rate. The tree with 9 terminal nodes results in the lowest cross-validation error rate with 50 cross-validation errors.
```{r class-tree-pruning2, echo=TRUE, include=TRUE}
prune.carseats<-prune.misclass(tree.carseats, best=cv.carseats$size[which.min(cv.carseats$dev)]) # prune.misclass() function in order to prune the tree to obtain the nine-node tree, prune.misclass=prune.tree(method="misclass")
plot(prune.carseats)
text(prune.carseats, pretty=0)
tree.pred<-predict(prune.carseats, Carseats[-train,], type="class")
table(tree.pred, High.test)
accuracy<-(94+60)/(94+24+22+60); accuracy
confusionMatrix(tree.pred, as.factor(High.test))
```
Now 77% of the test observations are correctly classified, so not only has the pruning process produced a more interpretable tree, but it has also improved the classification accuracy.
```{r class-tree-pruning3, echo=TRUE, include=TRUE}
prune.carseats<-prune.misclass(tree.carseats, best=15) # increased value of best
plot(prune.carseats)
text(prune.carseats, pretty=0)
tree.pred<-predict(prune.carseats, Carseats[-train,],type="class")
table(tree.pred, High.test)
accuracy<-(86+62)/(86+22+30+62); accuracy
confusionMatrix(tree.pred, as.factor(High.test))
```
We obtain a larger pruned tree with lower classification accuracy(74%).
## __2. Regression Tree__
```{r reg-tree, echo=TRUE, include=TRUE}
set.seed(1)
train<-sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston<-tree(medv~., Boston[train,])
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty=0, cex=.7)
```
We now use the cv.tree() function to see whether pruning the tree will improve performance.
```{r reg-tree-pruning, echo=TRUE, include=TRUE}
cv.boston<-cv.tree(tree.boston, FUN=prune.tree, K=10)
par(mfrow=c(1,2))
plot(cv.boston$size, cv.boston$dev, type="b")
# In this case, the more complex tree is selected by cross-validation.
plot(cv.boston$k, cv.boston$dev, type="b")
par(mfrow=c(1,1))
prune.boston<-prune.tree(tree.boston, best=5) # We still want to prune the tree
plot(prune.boston)
text(prune.boston, pretty=0, cex=.7)
## MSE of Pruned tree
tree.pred<-predict(prune.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(tree.pred, boston.test)
abline(0,1)
mean((tree.pred-boston.test)^2)
## MSE of Unpruned tree
tree.pred<-predict(tree.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(tree.pred, boston.test)
abline(0,1)
mean((tree.pred-boston.test)^2)
```
The test set MSE associated with the regression tree is 25.05. The square root of the MSE is therefore around 5.005, indicating that this model leads to test predictions that are within around $5,005 of the true median home value for the suburb.
Note that the MSE of the unpruned tree is smaller than that of pruned tree, which can be explained from the cross-validation error plot.
* * *
## __Bagging and Random Forests__
Bagging is simply a special case of a random forest with m=p. Therefore, the randomForest() function can be used to perform both random forests and bagging.
```{r dfsplit, echo=TRUE, include=TRUE}
set.seed(1)
train<-sample(1:nrow(Boston), nrow(Boston)/2)
```
### Bagging
```{r bagging, echo=TRUE, include=TRUE}
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,],
mtry=13, # Number of variables randomly sampled as candidates at each split
ntree=25) # Number of trees to grow
bag.boston
bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
```
The argument mtry=13 indicates that all 13 predictors should be considered for each split of the tree - in other words, that bagging should be done.
### Random Forest
Growing a random forest proceeds in exactly the same way, except that we use a smaller value of the mtry argument. By default, randomForest() uses __p/3 variables__ when building a random forest of __regression trees__, and __sqrt(p) variables__ when building a random forest of __classification trees__. Here we use mtry=6.
```{r rf, echo=TRUE, include=TRUE}
set.seed(1)
rf.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25,
importance=TRUE) # Number of trees to grow
rf.boston
rf.pred<-predict(rf.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(rf.pred, boston.test)
mean((rf.pred-boston.test)^2)
importance(rf.boston)
```
The random forests yield an improvement over bagging in terms of MSE. Using the importance function, we can view the importance of each variable.
cv.boston<-cv.tree(tree.boston, FUN=prune.tree, K=10)
par(mfrow=c(1,2))
plot(cv.boston$size, cv.boston$dev, type="b")
# In this case, the more complex tree is selected by cross-validation.
plot(cv.boston$k, cv.boston$dev, type="b")
par(mfrow=c(1,1))
prune.boston<-prune.tree(tree.boston, best=5) # We still want to prune the tree
plot(prune.boston)
text(prune.boston, pretty=0, cex=.7)
## MSE of Pruned tree
tree.pred<-predict(prune.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(tree.pred, boston.test)
abline(0,1)
mean((tree.pred-boston.test)^2)
## MSE of Unpruned tree
tree.pred<-predict(tree.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(tree.pred, boston.test)
abline(0,1)
mean((tree.pred-boston.test)^2)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston[train,],
mtry=13, # Number of variables randomly sampled as candidates at each split
ntree=25) # Number of trees to grow
bag.boston
bag.pred<-predict(bag.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(bag.pred, boston.test)
mean((bag.pred-boston.test)^2)
set.seed(1)
rf.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25,
importance=TRUE) # Number of trees to grow
rf.boston
rf.pred<-predict(rf.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(rf.pred, boston.test)
mean((rf.pred-boston.test)^2)
importance(rf.boston)
set.seed(1)
rf.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25,
importance=TRUE) # Number of trees to grow
rf.boston
rf.pred<-predict(rf.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(rf.pred, boston.test)
mean((rf.pred-boston.test)^2)
importance(rf.boston)
set.seed(1)
rf.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25,
importance=TRUE) # Number of trees to grow
rf.boston
rf.pred<-predict(rf.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(rf.pred, boston.test)
mean((rf.pred-boston.test)^2)
importance(rf.boston)
importance(rf.boston)
varImpPlot(rf.boston)
importance(rf.boston)
varImpPlot(rf.boston)
importance(rf.boston)
varImpPlot(rf.boston)
install.packages("gbm")
library(gbm)
set.seed(1)
boost.boston<-gbm(medv~., data=Boston[train,],
distribution="gaussian",
n.trees=5000, # number of trees
interaction.depth=4) # the depth of each tree
summary(boost.boston)
set.seed(1)
boost.boston<-gbm(medv~., data=Boston[train,],
distribution="gaussian",
n.trees=5000, # number of trees
interaction.depth=4) # the depth of each tree
summary(boost.boston)
par(mfrow=c(1,2))
plot(boost.boston, i="rm")
plot(boost.boston, i="lstat")
set.seed(1)
boost.boston<-gbm(medv~., data=Boston[train,],
distribution="gaussian",
n.trees=5000, # number of trees
interaction.depth=4) # the depth of each tree
summary(boost.boston)
boost.pred<-predict(boost.boston, newdata=Boston[-train,],
n.trees=5000)
mean((boost.pred-boston.test)^2)
?gbm
set.seed(1)
boost.boston<-gbm(medv~., data=Boston[train,],
distribution="gaussian",
n.trees=5000, # number of trees (default:100)
interaction.depth=4) # the depth of each tree
summary(boost.boston)
boost.pred<-predict(boost.boston, newdata=Boston[-train,],
n.trees=5000)
mean((boost.pred-boston.test)^2)
## shrinkage parameter
boost.boston<-gbm(medv~., data=Boston[train,],
distribution="gaussian",
n.trees=5000, # number of trees (default:100)
interaction.depth=4, # the depth of each tree
shrinkage=0.2, # shrinkage parameter applied to each tree in the expansion. Also know as the learning rate or step-size reduction. A smaller learning rate typically requires more trees. Default is 0.1
verbose=F)
boost.pred<-predict(boost.boston, newdata=Boston[-train,],
n.trees=5000)
mean((boost.pred-boston.test)^2)
set.seed(1)
boost.boston<-gbm(medv~., data=Boston[train,],
distribution="gaussian",
n.trees=5000, # number of trees (default:100)
interaction.depth=4) # the depth of each tree
summary(boost.boston)
boost.pred<-predict(boost.boston, newdata=Boston[-train,],
n.trees=5000)
mean((boost.pred-boston.test)^2)
## shrinkage parameter (learning rate)
boost.boston<-gbm(medv~., data=Boston[train,],
distribution="gaussian",
n.trees=5000, # number of trees (default:100)
interaction.depth=4, # the depth of each tree
shrinkage=0.2, # shrinkage parameter applied to each tree in the expansion. Also know as the learning rate or step-size reduction. A smaller learning rate typically requires more trees. Default is 0.1
verbose=F)
boost.pred<-predict(boost.boston, newdata=Boston[-train,],
n.trees=5000)
mean((boost.pred-boston.test)^2)
set.seed(1)
boost.boston<-gbm(medv~., data=Boston[train,],
distribution="gaussian",
n.trees=5000, # number of trees (default:100)
interaction.depth=4) # the depth of each tree
summary(boost.boston)
boost.pred<-predict(boost.boston, newdata=Boston[-train,],
n.trees=5000)
mean((boost.pred-boston.test)^2)
## shrinkage parameter (learning rate)
boost.boston<-gbm(medv~., data=Boston[train,],
distribution="gaussian",
n.trees=5000, # number of trees (default:100)
interaction.depth=4, # the depth of each tree
shrinkage=0.2, # shrinkage parameter applied to each tree in the expansion. Also know as the learning rate or step-size reduction. A smaller learning rate typically requires more trees. Default is 0.1
verbose=F)
boost.pred<-predict(boost.boston, newdata=Boston[-train,],
n.trees=5000)
mean((boost.pred-boston.test)^2)
set.seed(1)
rf.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25,
importance=TRUE) # Number of trees to grow
rf.boston
rf.pred<-predict(rf.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(rf.pred, boston.test)
mean((rf.pred-boston.test)^2)
set.seed(1)
rf.boston<-randomForest(medv~., data=Boston[train,],
mtry=6, # Number of variables randomly sampled as candidates at each split
ntree=25,
importance=TRUE) # Number of trees to grow
rf.boston
rf.pred<-predict(rf.boston, newdata=Boston[-train,])
boston.test<-Boston[-train, "medv"]
plot(rf.pred, boston.test)
mean((rf.pred-boston.test)^2)
